
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  
    <title>jiayi797的专栏</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="jiayi797">
    

    
    <meta property="og:type" content="website">
<meta property="og:title" content="jiayi797的专栏">
<meta property="og:url" content="http://yoursite.com/child/page/2/index.html">
<meta property="og:site_name" content="jiayi797的专栏">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="jiayi797的专栏">

    
    <link rel="alternative" href="/atom.xml" title="jiayi797的专栏" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<link rel="dns-prefetch" href="//cdn.bootcss.com" />
<link rel="dns-prefetch" href="//cdn.mathjax.org" />

  <body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="jiayi797的专栏" title="jiayi797的专栏"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="jiayi797的专栏">jiayi797的专栏</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="搜索" />
						<input type="hidden" name="q" value="site:yoursite.com/child">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main">

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/03/16/java学习笔记-继承/" title="java学习笔记——继承" itemprop="url">java学习笔记——继承</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="jiayi797" target="_blank" itemprop="author">jiayi797</a>
		
  <p class="article-time">
    <time datetime="2017-03-16T02:58:32.000Z" itemprop="datePublished"> 发表于 2017-03-16</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h1 id="super关键字">1. super关键字</h1><h2 id="子类override-覆盖-父类的函数">1.1. 子类override(覆盖)父类的函数</h2><p>override时，使用<code>super</code>调用父类的方法。</p>
<pre><code>//超类，员工
class Employee{
    private String name;
    private double salary;//薪水
    private Date hireDay;
    public double getSalary(){
        return salary;
    }
}
//经理
class Manage extends Employee{
    private double bonus;//奖金
    public double getSalary(){//对原来的getSalary进行override
        return this.bonus + super.getSalary();//super调用父类的getSalary()方法
    }
}
</code></pre><h2 id="super在构造器中的应用">1.2. super在构造器中的应用</h2><pre><code>public Manager(String n, double s, int year, int mouth, int day){
    super(n,s,year,mouth,day);//调用超类Employee中对应参数的构造器
    bonus = 0;
}
</code></pre><h1 id="多态">2. 多态</h1><p>一个对象变量可以指示多种实际类型的现象————多态（polymorphism）<br>在运行时自动选择调用那个方法的现象————动态绑定（dynamic binding）</p>
<p>例如：</p>
<pre><code>Employee e = new Employee();
e = new Manage(); // is ok
</code></pre><p>此时，对象变量<code>e</code>也可以引用<code>Manager</code>的对象。</p>
<pre><code>Manage boss = new Manage(...);
Employee[] staff = new Employee();
</code></pre><h1 id="调用对象方法的执行过程">3. 调用对象方法的执行过程</h1><ol>
<li>编译器查看对象的声明类型和方法名。假设调用<code>x.f(param)</code>，且隐式参数x声明为C类的对象。编译器一一列举所有C类中名为f的方法和其超类中访问属性为public且名为f的方法。</li>
<li>编译器查看调用方法时提供的参数类型。这个过程叫做重载解析（overloading resolution）</li>
<li>如果是private方法、static方法、final方法或者构造器，那么编译器会准确地知道应该调用哪个方法（编译器可以在编译阶段就完成绑定），这种调用方式叫<strong>静态绑定</strong>(static binding)。与此对应的是，调用的方法依赖于隐式参数的实际类型（动态绑定是指编译器在编译阶段不知道要调用哪个方法，直到运行时才能确定），并在运行时实现<strong>动态绑定</strong>。</li>
<li>程序运行时，若是动态绑定调用方法，那就先从本类中寻找，否则从超类中寻找。（每个类都有一个方法表method table）</li>
</ol>
<h1 id="阻止继承：final类和方法">4. 阻止继承：final类和方法</h1><p>不允许扩展（被继承）的类被称为final类。</p>
<p>例如下例子中的Executive类就不能有子类。</p>
<pre><code>final class Executive extends Manager{
    ...
}
</code></pre><p>类中的方法也可以final,那么子类就不能覆盖这个方法。</p>
<pre><code>class Employee{
    ...
    public final String getName(){
        return name;
    }
    ...
}
</code></pre><h1 id="抽象类，abstruct">5. 抽象类，abstruct</h1><p>用于表示某种很抽象的、上层的、更通用的类。例如图中的</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/Java/">Java</a>
</div>


</div>




<div class="comments-count">
	
	  	<span></span>
		<a href="/2017/03/16/java学习笔记-继承/#comments" class="ds-thread-count comments-count-link" data-thread-key="2017/03/16/java学习笔记-继承/" data-count-type="comments">&nbsp;</a>
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/03/15/GBDT/" title="GBDT" itemprop="url">GBDT</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="jiayi797" target="_blank" itemprop="author">jiayi797</a>
		
  <p class="article-time">
    <time datetime="2017-03-15T05:31:46.000Z" itemprop="datePublished"> 发表于 2017-03-15</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>之前的AdaBoostDTree的误差函数是指数型的。GBDT的误差函数是任意指定的。<br>GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。</p>
<h1 id="GBDT的误差函数">1. GBDT的误差函数</h1><p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-15-13-49-01.png" alt=""> </p>
<h1 id="目标">2. 目标</h1><ol>
<li>求函数h(x)的形式</li>
<li>求h(x)的步长η</li>
</ol>
<h1 id="回归问题求解目标">3. 回归问题求解目标</h1><p>对于回归（regression）问题，误差函数一般采用平方误差。即：</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-15-13-55-15.png" alt=""> </p>
<p>为了进一步求解，我们对上式进行taylor展开，即：</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-15-13-58-42.png" alt=""> </p>
<p>其中：</p>
<ul>
<li>左边一项$err(S_n,y_n)$是常量（因为$S_n$、$y_n$都已知）</li>
<li>右边一项应该对s求导，并在sn这点取导数值（$error=(s-y)^2$求导之后得到$2(s-y)$）</li>
</ul>
<p>那么，上式等于：</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-15-14-04-27.png" alt=""> </p>
<h2 id="求h-x">3.1. 求h(x)</h2><p>为了让上式最小化，那么貌似$|h(x)|$无穷大即可实现，这不科学！于是我们要对$h(x)$的大小进行限制（类似归一化）————加入惩罚项$(h(x_n))^2$，即将上式变为：</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-15-14-19-46.png" alt=""> </p>
<p>而上式可变为：</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-15-14-20-27.png" alt=""> </p>
<p>其中的$(s_n-y_n)^2$是常数，记为constant</p>
<p>那么新的目标就变为：用$x_n$和$y_n-s_n$做一个regression即可。即：</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-15-14-27-31.png" alt=""> </p>
<p>经过penalize一番折腾之后，h终于有个像模像样的形式了：即regression with residuals（残差）。</p>
<h2 id="求步长η">3.2. 求步长η</h2><p>需要求得一个η，使得下式最小化：</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-15-15-21-27.png" alt=""> </p>
<p>为了方便计算，我们将平方内的项取负：</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-15-15-22-55.png" alt=""> </p>
<p>上式的$y_n-s_n$即residuals（残差）.这是一个单变量的线性回归问题，其中输入是用gt转换后的数据，输出是残差(residual)。</p>
<h1 id="GBDT算法">4. GBDT算法</h1><p>输出：$\sum_t^T\alpha_t g_t(x)$，即一堆权重$\alpha_t$和一堆决策树$g_t(x)$<br>步骤：<br><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-15-15-32-42.png" alt=""> </p>
<p>1）利用C&amp;RT去学{x, yn-sn}，保留这一轮学出来的树gt(x)</p>
<p>2）再求{gt(x), residual}线性回归，最小化目标函数求出来ita</p>
<p>3）更新sn</p>
<p>学习足够多次数后，返回组合的GBDT。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
</div>


</div>




<div class="comments-count">
	
	  	<span></span>
		<a href="/2017/03/15/GBDT/#comments" class="ds-thread-count comments-count-link" data-thread-key="2017/03/15/GBDT/" data-count-type="comments">&nbsp;</a>
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/03/14/jacman-hexo目录改成浮动/" title="jacman/hexo目录改成浮动" itemprop="url">jacman/hexo目录改成浮动</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="jiayi797" target="_blank" itemprop="author">jiayi797</a>
		
  <p class="article-time">
    <time datetime="2017-03-14T14:29:18.000Z" itemprop="datePublished"> 发表于 2017-03-14</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h1 id="添加样式支持">1. 添加样式支持</h1><p>为了不吧原先的像是文件搞得太乱，这里，添加子集的样式文件。<br>首先，在样式文件的<code>source</code>文件夹下找到<code>css</code>文件夹，打开<code>style.styl</code>文件，在最后添加：</p>
<p><code>@import &quot;_my/mycss&quot;;</code> </p>
<h1 id="新建自定义样式">2. 新建自定义样式</h1><p>找到样式文件夹<code>css</code> 新建<code>_my</code>文件夹，在其中新建<code>mycss.sty</code>l文件，之后就可以按照<code>stylus</code>的格式自定义样式了。</p>
<h1 id="设置toc浮动">3. 设置toc浮动</h1><p>给mycss.sty添加：</p>
<pre><code>#toc
 line-height 1.2em
 font-size 0.8em
 backgroud-color #fff
 float right
 position fixed
 right 30em
 top 20em
</code></pre><h1 id="存在的问题">4. 存在的问题</h1><p>暂不支持自相应。</p>
<h1 id="参考文献">5. 参考文献</h1><ol>
<li><a href="https://segmentfault.com/a/1190000003846777" target="_blank" rel="external">Hexo博客主题NexT使用自定义的CSS样式</a></li>
</ol>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/瞎折腾/">瞎折腾</a>
</div>


</div>




<div class="comments-count">
	
	  	<span></span>
		<a href="/2017/03/14/jacman-hexo目录改成浮动/#comments" class="ds-thread-count comments-count-link" data-thread-key="2017/03/14/jacman-hexo目录改成浮动/" data-count-type="comments">&nbsp;</a>
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/03/14/java学习笔记-类与对象/" title="java学习笔记----类与对象" itemprop="url">java学习笔记----类与对象</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="jiayi797" target="_blank" itemprop="author">jiayi797</a>
		
  <p class="article-time">
    <time datetime="2017-03-14T01:56:21.000Z" itemprop="datePublished"> 发表于 2017-03-14</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>本文主要总结了一些自己不太熟悉的概念。</p>
<h1 id="对象与对象变量">1. 对象与对象变量</h1><pre><code>Date birthday = new Date();
</code></pre><p>对象变量：birthday<br>对象：右边的部分</p>
<p>一个对象变量并没有实际包含一个对象，仅仅是引用一个对象。</p>
<p>可以显示地将对象变量设置为<code>null</code>，表明这个对象变量目前没有引用任何对象：</p>
<pre><code>birthday = null;
</code></pre><h1 id="隐式参数与显式参数">2. 隐式参数与显式参数</h1><p>例，methodName()是类class1的方法，</p>
<pre><code>calss class1{
int a;
    public void methodName(int b){
    this.a = b ;
}
</code></pre><ul>
<li>显式参数(explicit)：括号里面的，例如double pName</li>
<li>隐式参数(implicit)：出现在方法名前的class1类对象<br>– 关键词<code>this</code>表示隐式参数。例如<code>this.a</code></li>
</ul>
<h1 id="封装">3. 封装</h1><p>不能编写返回<code>引用可变对象</code>的访问器方法！例如：</p>
<pre><code>class class1{
    private Date a;
    public Date get(){
        return Date a; //会破坏封装性！
    }
}
</code></pre><p>以上操作破坏了<code>a</code>的私有性。</p>
<p>改正方法：克隆（clone）</p>
<pre><code>class class1{
    private Date a;
    public Date get(){
        return Date a.clone(); //使用clone()
    }
}
</code></pre><h1 id="final实例域">4. final实例域</h1><pre><code>class class1{
    private final String name;
}
</code></pre><p>final域的特征：</p>
<ol>
<li>构造对象时，必须初始化final</li>
<li>后面操作中，不能再改动</li>
<li>但并不等于常量！</li>
<li>属于对象，并不是类！</li>
</ol>
<h1 id="static静态">5. static静态</h1><h2 id="static域">5.1. static域</h2><ol>
<li>每个类只能有一个static域</li>
<li>同一类的所有对象共享一个static域</li>
<li>即使没有对象，static域也存在。它属于类，不属于任何一个对象</li>
</ol>
<pre><code>class Employee{
    private static int nextId = 1;
    private int id;
}
</code></pre><h2 id="static常量">5.2. static常量</h2><ol>
<li>如下例，在程序中，可以使用<code>Math.PI</code>来获取这个常量。</li>
</ol>
<pre><code>public class Math{
    public static final double PI = 3.14;
}
</code></pre><h2 id="static方法">5.3. static方法</h2><pre><code>Math.pow(x,a)
</code></pre><ol>
<li>不使用任何对象；</li>
<li>不能操作实例域（即类内的非static方法和变量），因为它不能操作对象；</li>
<li>可以访问自身类的static域；</li>
<li>对象也可以调用static方法。</li>
</ol>
<p>在下面两种情况使用静态方法：</p>
<ol>
<li>一个方法不需要访问对象；</li>
<li>一个方法只需要访问类的static域。</li>
</ol>
<h2 id="工厂方法">5.4. 工厂方法</h2><p>工厂方法是静态方法的一种常见用途。<br>例如，<code>NumberFormat</code>使用工厂方法(而不是构造器)产生<strong>不同风格</strong>的格式对象。</p>
<pre><code>NumberFormat a = NumberFormat.getSytleA();
NumberFormat b = NumberFormat.getStyleB();
</code></pre><h2 id="main方法">5.5. main方法</h2><pre><code>public class Application{
    public static void main(String[] args){
        // construct objects here
    }
}
</code></pre><ol>
<li>每一个类可以有一个main方法，用来单元测试；</li>
<li>多个类被调用时，只会执行一个main方法；</li>
</ol>
<h1 id="初始化块">6. 初始化块</h1><p>构造对象时，先运行初始化块，才运行构造器主体部分。</p>
<pre><code>class Employee{
    private static int nextId;
    private int id;
    //初始化块
    {
        id=nextId;
    }
}
</code></pre><p>如果对类的静态域进行初始化的代码比较复杂，就可以使用静态的初始化块：</p>
<pre><code>static{
    Random generator = new Random();
    nextId = generator.nextId(10000);
}
</code></pre><p>类（！！！不是对象）第一次加载时，将会进行static域的初始化。</p>
<h1 id="初始化数据域的三种方法">7. 初始化数据域的三种方法</h1><ol>
<li>在构造器中设置值</li>
<li>在声明中赋值</li>
<li>在初始化块中赋值</li>
</ol>
<h1 id="类的初始化顺序">8. 类的初始化顺序</h1><p>对于静态变量、静态初始化块、变量、初始化块、构造器，它们的初始化顺序依次是（静态变量、静态初始化块）&gt;（变量、初始化块）&gt;构造器。</p>
<p>例如，</p>
<pre><code>public class InitialOrderTest {   
    // 静态变量   
    public static String staticField = &quot;静态变量&quot;;   
    // 变量   
    public String field = &quot;变量&quot;;   
    // 静态初始化块   
    static {   
        System.out.println(staticField);   
        System.out.println(&quot;静态初始化块&quot;);   
    }   
    // 初始化块   
    {   
        System.out.println(field);   
        System.out.println(&quot;初始化块&quot;);   
    }   
    // 构造器   
    public InitialOrderTest() {   
        System.out.println(&quot;构造器&quot;);   
    }   
    public static void main(String[] args) {   
        new InitialOrderTest();   
    }   
}  
</code></pre><p>运行以上代码，我们会得到如下的输出结果： </p>
<pre><code>静态变量
静态初始化块
变量
初始化块
构造器
</code></pre><p>对于继承的情况：</p>
<pre><code>class Parent {   
    // 静态变量   
    public static String p_StaticField = &quot;父类--静态变量&quot;;   
    // 变量   
    public String p_Field = &quot;父类--变量&quot;;   

    // 静态初始化块   
    static {   
        System.out.println(p_StaticField);   
        System.out.println(&quot;父类--静态初始化块&quot;);   
    }   

    // 初始化块   
    {   
        System.out.println(p_Field);   
        System.out.println(&quot;父类--初始化块&quot;);   
    }   

    // 构造器   
    public Parent() {   
        System.out.println(&quot;父类--构造器&quot;);   
    }   
}   

public class SubClass extends Parent {   
    // 静态变量   
    public static String s_StaticField = &quot;子类--静态变量&quot;;   
    // 变量   
    public String s_Field = &quot;子类--变量&quot;;   
    // 静态初始化块   
    static {   
        System.out.println(s_StaticField);   
        System.out.println(&quot;子类--静态初始化块&quot;);   
    }   
    // 初始化块   
    {   
        System.out.println(s_Field);   
        System.out.println(&quot;子类--初始化块&quot;);   
    }   

    // 构造器   
    public SubClass() {   
        System.out.println(&quot;子类--构造器&quot;);   
    }   

    // 程序入口   
    public static void main(String[] args) {   
        new SubClass();   
    }   
}  
</code></pre><p>运行一下上面的代码，结果马上呈现在我们的眼前： </p>
<pre><code>父类--静态变量
父类--静态初始化块
子类--静态变量
子类--静态初始化块
父类--变量
父类--初始化块
父类--构造器
子类--变量
子类--初始化块
子类--构造器
</code></pre><p>总得来说，是先静态后变量，先父类后子类</p>
<h1 id="其他重点">9. 其他重点</h1><ol>
<li>基于类的访问权限：一个方法可以访问所属类的所有私有数据。</li>
<li>java的值引用（基本数据类型、对象引用）</li>
<li>如果类中提供了至少一个有参构造器，而没有无参构造器，则在构造无参对象时会出错。</li>
</ol>
<h1 id="Java类库中的GregorianCalendar类-（删除本节）">10. Java类库中的GregorianCalendar类 （删除本节）</h1><h2 id="纪元">10.1. 纪元</h2><p>时间是用距离一个固定时间点的毫秒数表示的，这个点就是纪元(epoch)。</p>
<h2 id="时间与日历">10.2. 时间与日历</h2><p>为了将<strong>时间</strong>与<strong>日历</strong>分开，标准Java类库分别包含两个类：</p>
<ul>
<li>Date类：用来表示时间点的类；</li>
<li>GregorianCalendar类：用来表示公历法的类；（通过它还有一个扩展类——Calendar类，描述了日历的一般属性）</li>
</ul>
<h3 id="Date类">10.2.1. Date类</h3><p>用来表示时间的类；</p>
<p>只有少量的方法，例如比较两个时间点before(),after()：</p>
<pre><code>doday.before(birthday)
</code></pre><h3 id="GregorianCalendar类">10.2.2. GregorianCalendar类</h3><p>常见方法：</p>
<p><code>new GregorianCalendar()</code>，构造新的对象，用于表示对象构造时的日期和时间；</p>
<p>例如:</p>
<pre><code>GregorianCalendar g1 = new GregorianCalendar();
</code></pre><p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-14-10-47-16.png" alt=""> </p>
<p><code>new GregorianCalendar(1999,11,31)</code>，提供年月日构造一个表示特定日期午夜的日历对象。（月份从0开始计数，11表示12月）</p>
<p><code>new GregorianCalendar(1991,Calendar.DECEMBER,31)</code>,与上等价</p>
<p><code>new GregorianCalendar(1991,Calendar.DECEMBER,31,23,59,59)</code>,设置时间</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/Java/">Java</a>
</div>


</div>




<div class="comments-count">
	
	  	<span></span>
		<a href="/2017/03/14/java学习笔记-类与对象/#comments" class="ds-thread-count comments-count-link" data-thread-key="2017/03/14/java学习笔记-类与对象/" data-count-type="comments">&nbsp;</a>
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/03/11/O2O优惠券预测——对第一名的思路源码分析（二）/" title="O2O优惠券预测——对第一名的思路源码分析（二）" itemprop="url">O2O优惠券预测——对第一名的思路源码分析（二）</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="jiayi797" target="_blank" itemprop="author">jiayi797</a>
		
  <p class="article-time">
    <time datetime="2017-03-11T08:58:16.000Z" itemprop="datePublished"> 发表于 2017-03-11</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>本文主要针对天池大数据竞赛之“O2O优惠券使用预测”的冠军队伍的思路和源码分析。在此感谢无私的前辈(诗人都藏在水底)[<a href="https://github.com/wepe/O2O-Coupon-Usage-Forecast]。" target="_blank" rel="external">https://github.com/wepe/O2O-Coupon-Usage-Forecast]。</a></p>
<p>本文主要对模型训练<code>xgb.py</code> 做一些详细的分析。</p>
<p>文件：O2O-Coupon-Usage-Forecast/code/wepon/season one </p>
<p><code>xgb.py</code> 训练xgboost模型，生成特征重要性文件，生成预测结果。单模型第一赛季A榜AUC得分0.798.</p>
<h1 id="import包">1. import包</h1><p>首先作者import xgboost,因此我们需要安装一下它。</p>
<p>XGBoost是数据挖掘中用到一个新型的数据分析包，相对其它Boosting模型更加高效。</p>
<p>安装教程<a href="http://www.jianshu.com/p/11f9229b0ecd" target="_blank" rel="external">xgboost install on windows</a></p>
<h1 id="导入数据">2. 导入数据</h1><pre><code>#将数据集导入
dataset1 = pd.read_csv(&apos;data/dataset1.csv&apos;)
dataset2 = pd.read_csv(&apos;data/dataset2.csv&apos;)
dataset3 = pd.read_csv(&apos;data/dataset3.csv&apos;)
</code></pre><p><code>dataset1、dataset2</code>有56个特征，图是前十个。</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-12-22-11-30.png" alt=""> </p>
<p><code>dataset3</code>有57个特征</p>
<pre><code>#将dataset1的label列的-1都换成0
dataset1.label.replace(-1,0,inplace=True)
dataset2.label.replace(-1,0,inplace=True)

#删除重复项
dataset1.drop_duplicates(inplace=True)
dataset2.drop_duplicates(inplace=True)
dataset3.drop_duplicates(inplace=True)
</code></pre><p>将dataset1和dataset2连起来</p>
<pre><code>dataset12 = pd.concat([dataset1,dataset2],axis=0)
</code></pre><p>dataset1_y赋值为dataset1的label列</p>
<pre><code>dataset1_y = dataset1.label
</code></pre><p>删除dataset1的’user_id’,’label’,’day_gap_before’,’day_gap_after’字段，赋值给dataset1_x</p>
<pre><code>dataset1_x = dataset1.drop([&apos;user_id&apos;,&apos;label&apos;,&apos;day_gap_before&apos;,&apos;day_gap_after&apos;],axis=1)  # &apos;day_gap_before&apos;,&apos;day_gap_after&apos; cause overfitting, 0.77


dataset2_y = dataset2.label
dataset2_x = dataset2.drop([&apos;user_id&apos;,&apos;label&apos;,&apos;day_gap_before&apos;,&apos;day_gap_after&apos;],axis=1)
dataset12_y = dataset12.label
dataset12_x = dataset12.drop([&apos;user_id&apos;,&apos;label&apos;,&apos;day_gap_before&apos;,&apos;day_gap_after&apos;],axis=1)
dataset3_preds = dataset3[[&apos;user_id&apos;,&apos;coupon_id&apos;,&apos;date_received&apos;]]
dataset3_x = dataset3.drop([&apos;user_id&apos;,&apos;coupon_id&apos;,&apos;date_received&apos;,&apos;day_gap_before&apos;,&apos;day_gap_after&apos;],axis=1)
</code></pre><p>用shape属性来显示数据的格式</p>
<pre><code>print dataset1_x.shape,dataset2_x.shape,dataset3_x.shape
</code></pre><p>若输出：(8618,36) 表示这个表格有8618行和36列的数据，其中dimensions[0]为8618，dimensions[1]为36</p>
<h1 id="加载数据到xgboost">3. 加载数据到xgboost</h1><p>dataset1、dateset2、dateset3 为xgb的DMatrix</p>
<pre><code>dataset1 = xgb.DMatrix(dataset1_x,label=dataset1_y)
dataset2 = xgb.DMatrix(dataset2_x,label=dataset2_y)
dataset12 = xgb.DMatrix(dataset12_x,label=dataset12_y)
dataset3 = xgb.DMatrix(dataset3_x)
</code></pre><p>参考文献<a href="http://blog.csdn.net/sb19931201/article/details/52557382" target="_blank" rel="external">xgboost入门与实战（原理篇）</a></p>
<h1 id="设置参数">4. 设置参数</h1><pre><code>params={&apos;booster&apos;:&apos;gbtree&apos;,
        &apos;objective&apos;: &apos;rank:pairwise&apos;,
        &apos;eval_metric&apos;:&apos;auc&apos;,
        &apos;gamma&apos;:0.1,
        &apos;min_child_weight&apos;:1.1,
        &apos;max_depth&apos;:5,
        &apos;lambda&apos;:10,
        &apos;subsample&apos;:0.7,
        &apos;colsample_bytree&apos;:0.7,
        &apos;colsample_bylevel&apos;:0.7,
        &apos;eta&apos;: 0.01,
        &apos;tree_method&apos;:&apos;exact&apos;,
        &apos;seed&apos;:0,
        &apos;nthread&apos;:12
        }
</code></pre><h1 id="训练模型">5. 训练模型</h1><pre><code>model = xgb.train(params,dataset12,num_boost_round=3500,evals=watchlist)    
</code></pre><h1 id="预测测试集">6. 预测测试集</h1><pre><code>dataset3_preds[&apos;label&apos;] = model.predict(dataset3)
dataset3_preds.label = MinMaxScaler().fit_transform(dataset3_preds.label)
dataset3_preds.sort_values(by=[&apos;coupon_id&apos;,&apos;label&apos;],inplace=True)
dataset3_preds.to_csv(&quot;xgb_preds.csv&quot;,index=None,header=None)
print dataset3_preds.describe()
</code></pre><h1 id="保存特征评分">7. 保存特征评分</h1><pre><code>feature_score = model.get_fscore()
feature_score = sorted(feature_score.items(), key=lambda x:x[1],reverse=True)
fs = []
for (key,value) in feature_score:
    fs.append(&quot;{0},{1}\n&quot;.format(key,value))

with open(&apos;xgb_feature_score.csv&apos;,&apos;w&apos;) as f:
    f.writelines(&quot;feature,score\n&quot;)
    f.writelines(fs)
</code></pre><h1 id="总结">8. 总结</h1><p>这次算是对自己之前的各种理论知识进行了一次梳理，感觉平时过于注重算法的研究，并没有注意到宏观上的操作。以后要多加注意</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/o2o优惠券使用预测/">o2o优惠券使用预测</a>
</div>


</div>




<div class="comments-count">
	
	  	<span></span>
		<a href="/2017/03/11/O2O优惠券预测——对第一名的思路源码分析（二）/#comments" class="ds-thread-count comments-count-link" data-thread-key="2017/03/11/O2O优惠券预测——对第一名的思路源码分析（二）/" data-count-type="comments">&nbsp;</a>
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/03/08/O2O优惠券预测——对第一名的思路源码分析（一）/" title="O2O优惠券预测——对第一名的思路源码分析（一）" itemprop="url">O2O优惠券预测——对第一名的思路源码分析（一）</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="jiayi797" target="_blank" itemprop="author">jiayi797</a>
		
  <p class="article-time">
    <time datetime="2017-03-08T07:20:57.000Z" itemprop="datePublished"> 发表于 2017-03-08</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>感觉自己还是太渣，看了些许算法，并不知道有什么卵用。决定好好分析分析别人的思路，也许能够对我带来些许启发。</p>
<p>本文主要针对天池大数据竞赛之“O2O优惠券使用预测”的冠军队伍的思路和源码分析。在此感谢无私的前辈(诗人都藏在水底)[<a href="https://github.com/wepe/O2O-Coupon-Usage-Forecast]。" target="_blank" rel="external">https://github.com/wepe/O2O-Coupon-Usage-Forecast]。</a></p>
<p>本文主要对数据的抽取<code>extract_feature.py</code>做一些详细的分析。</p>
<h1 id="解决方案概述">1. 解决方案概述</h1><p>本赛题提供了用户线下消费和优惠券领取核销行为的纪录表，用户线上点击/消费和优惠券领取核销行为的纪录表，记录的时间区间是2016.01.01至2016.06.30,需要预测的是2016年7月份用户领取优惠劵后是否核销。根据这两份数据表，我们首先对数据集进行划分，然后提取了用户相关的特征、商家相关的特征，优惠劵相关的特征，用户与商家之间的交互特征，以及利用本赛题的leakage得到的其它特征（这部分特征在实际业务中是不可能获取到的）。最后训练了XGBoost，GBDT，RandomForest进行模型融合。</p>
<p><strong>源码分析</strong></p>
<p>第二赛季暂时没有平台，所以本文只对第一赛季的源码进行分析。</p>
<p>文件：O2O-Coupon-Usage-Forecast/code/wepon/season one </p>
<p>这个文件夹存放第一赛季的代码</p>
<ul>
<li><code>extract_feature.py</code>划分数据集，提取特征，生成训练集（dataset1和dataset2）和预测集（dataset3）。</li>
<li><code>xgb.py</code> 训练xgboost模型，生成特征重要性文件，生成预测结果。单模型第一赛季A榜AUC得分0.798.</li>
</ul>
<h1 id="import概述">2. import概述</h1><p>分析对象：extract_feature.py</p>
<h2 id="import包概述">2.1. import包概述</h2><pre><code>import pandas as pd
import numpy as np
from datetime import date
</code></pre><h2 id="pandas">2.2. pandas</h2><p>Pandas 是基于 NumPy (因此还要<code>import numpy</code>) 的一个非常好用的库，正如名字一样，人见人爱。之所以如此，就在于不论是读取、处理数据，用它都非常简单。Pandas提供了很多处理大数据的方法。我想是因为此，原作者才采用了它。</p>
<p>Pandas 有两种自己独有的基本数据结构。<code>Series</code> 和 <code>DataFrame</code>，它们让数据操作更简单了。</p>
<p>两种结构的属性和方法不再多阐述。见两份很好的参考文档：</p>
<ol>
<li><a href="http://wiki.jikexueyuan.com/project/start-learning-python/311.html" target="_blank" rel="external">Pandas 使用</a></li>
<li><a href="http://www.cnblogs.com/chaosimple/p/4153083.html" target="_blank" rel="external">十分钟搞定pandas</a></li>
<li><a href="http://dataunion.org/14261.html" target="_blank" rel="external">在Python中利用Pandas库处理大数据的简单介绍</a></li>
<li><a href="http://pandas.pydata.org/pandas-docs/stable/cookbook.html" target="_blank" rel="external">pandas官方文档</a></li>
<li><a href="http://www.cnblogs.com/pengsixiong/p/5050833.html" target="_blank" rel="external">pandas常见方法，中文</a></li>
</ol>
<p>大概知道了import包的内容后，我们正式开始看源码。</p>
<h2 id="注意">2.3. 注意</h2><ol>
<li>读取之前，请先把数据的表头项删除（也就是第一行的string）</li>
</ol>
<h1 id="读取数据集">3. 读取数据集</h1><p>总结：</p>
<table>
<thead>
<tr>
<th></th>
<th style="text-align:right">name</th>
<th style="text-align:right">content</th>
<th style="text-align:center">varName</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td style="text-align:right">ccf_offline_stage1_train</td>
<td style="text-align:right">用户线下消费和优惠券领取行为</td>
<td style="text-align:center">off_train</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:right">ccf_online_stage1_train</td>
<td style="text-align:right">用户线上点击/消费和优惠券领取行为</td>
<td style="text-align:center">on_train</td>
</tr>
<tr>
<td>3</td>
<td style="text-align:right">offline_stage1_test_revised</td>
<td style="text-align:right">用户O2O线下优惠券使用预测样本</td>
<td style="text-align:center">off_test</td>
</tr>
</tbody>
</table>
<h2 id="源码分析">3.1. 源码分析</h2><pre><code>#1754884 record,1053282 with coupon_id,9738 coupon. date_received:20160101~20160615,date:20160101~20160630, 539438 users, 8415 merchants

off_train = pd.read_csv(&apos;data/ccf_offline_stage1_train.csv&apos;,header=None)
off_train.columns = [&apos;user_id&apos;,&apos;merchant_id&apos;,&apos;coupon_id&apos;,&apos;discount_rate&apos;,&apos;distance&apos;,&apos;date_received&apos;,&apos;date&apos;]

#2050 coupon_id. date_received:20160701~20160731, 76309 users(76307 in trainset, 35965 in online_trainset), 1559 merchants(1558 in trainset)

off_test = pd.read_csv(&apos;data/ccf_offline_stage1_test_revised.csv&apos;,header=None,nrows=3000)
off_test.columns = [&apos;user_id&apos;,&apos;merchant_id&apos;,&apos;coupon_id&apos;,&apos;discount_rate&apos;,&apos;distance&apos;,&apos;date_received&apos;]

#11429826 record(872357 with coupon_id),762858 user(267448 in off_train)

on_train = pd.read_csv(&apos;data/ccf_online_stage1_train.csv&apos;,header=None,nrows=47000)
on_train.columns = [&apos;user_id&apos;,&apos;merchant_id&apos;,&apos;action&apos;,&apos;coupon_id&apos;,&apos;discount_rate&apos;,&apos;date_received&apos;,&apos;date&apos;]
</code></pre><p>读数据主要用了pandas的read_cvs方法. 为了快捷分析，我们限定只读取数据集的前7w、3k、47w行</p>
<h1 id="采集特征">4. 采集特征</h1><h2 id="主要特征">4.1. 主要特征</h2><p>总结：</p>
<table>
<thead>
<tr>
<th>term</th>
<th style="text-align:right">来源</th>
<th style="text-align:center">内容</th>
</tr>
</thead>
<tbody>
<tr>
<td>dataset3</td>
<td style="text-align:right">table3,off_test</td>
<td style="text-align:center">off_test数据</td>
</tr>
<tr>
<td>dataset2</td>
<td style="text-align:right">table2,off_train</td>
<td style="text-align:center">领券时期在20160515-20160615之间的</td>
</tr>
<tr>
<td>dataset1</td>
<td style="text-align:right">table2,off_train</td>
<td style="text-align:center">领券日期在20160414-20160514的</td>
</tr>
<tr>
<td>feature3</td>
<td style="text-align:right">table2,off_train</td>
<td style="text-align:center">消费data在20160315-20160630的，或领券日期在20160315-20160630但没有消费的</td>
</tr>
<tr>
<td>feature2</td>
<td style="text-align:right">table2,off_train</td>
<td style="text-align:center">消费日期在20160201-20160514的，或领券日期在20160201-20160514但没有消费的</td>
</tr>
<tr>
<td>feature1</td>
<td style="text-align:right">table2,off_train</td>
<td style="text-align:center">消费日期在20160101-20160413的，或领券日期在20160101-20160413但没有消费的</td>
</tr>
</tbody>
</table>
<p>这是滑窗的方法得到多份训练数据集，特征区间越小，得到的训练数据集越多。划分方式：</p>
<table>
<thead>
<tr>
<th>项目</th>
<th>预测区间（提取label）</th>
<th style="text-align:center">特征区间（提取feature）</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>领券了的</td>
<td style="text-align:center">消费了的+领券了没消费的</td>
</tr>
<tr>
<td>测试集</td>
<td>dataset3</td>
<td style="text-align:center">feature3</td>
</tr>
<tr>
<td>训练集1</td>
<td>dataset2</td>
<td style="text-align:center">feature2</td>
</tr>
<tr>
<td>训练集2</td>
<td>dataset1</td>
<td style="text-align:center">feature1</td>
</tr>
</tbody>
</table>
<p>上面这个表格很清晰地说明了原作者划分数据的方法.</p>
<h3 id="源码分析-1">4.1.1. 源码分析</h3><pre><code>#dataset3存放table3 的数据
dataset3 = off_test 

#feature3存放：筛出off_train中，以下四种情况：
#消费日期data在20160315-20160630的，
#或消费日期为空且领券日期在20160315-20160630的，
feature3 = off_train[
((off_train.date&gt;=&apos;20160315&apos;)&amp;(off_train.date&lt;=&apos;20160630&apos;))
|((off_train.date==&apos;null&apos;)&amp;(off_train.date_received&gt;=&apos;20160315&apos;)&amp;(off_train.date_received&lt;=&apos;20160630&apos;))]

#dataset2存放：从off_train筛出：领券时期在20160515-20160615之间的
dataset2 = off_train[
(off_train.date_received&gt;=&apos;20160515&apos;)&amp;off_train.date_received&lt;=&apos;20160615&apos;)]

#feature2存放：从off_train筛出：
#消费日期在20160201-20160514的，
#或领券日期在20160201-20160514但没有消费的
feature2 = off_train[(off_train.date&gt;=&apos;20160201&apos;)&amp;(off_train.date&lt;=&apos;20160514&apos;)|((off_train.date==&apos;null&apos;)&amp;(off_train.date_received&gt;=&apos;20160201&apos;)&amp;(off_train.date_received&lt;=&apos;20160514&apos;))]

#dataset1存放：从off_train筛出： 领券日期在20160414-20160514的
dataset1 = off_train[(off_train.date_received&gt;=&apos;20160414&apos;)&amp;(off_train.date_received&lt;=&apos;20160514&apos;)]

#feature1存放：从off_train筛出：
#消费日期在20160101-20160413的，或
#领券日期在20160101-20160413但没有消费的
feature1 = off_train[(off_train.date&gt;=&apos;20160101&apos;)&amp;(off_train.date&lt;=&apos;20160413&apos;)|((off_train.date==&apos;null&apos;)&amp;(off_train.date_received&gt;=&apos;20160101&apos;)&amp;(off_train.date_received&lt;=&apos;20160413&apos;))]
</code></pre><h2 id="其他特征">4.2. 其他特征</h2><h3 id="other-feature3">4.2.1. other_feature3</h3><table>
<thead>
<tr>
<th></th>
<th style="text-align:right">内容（都是来自测试集dataset3的数据）</th>
</tr>
</thead>
<tbody>
<tr>
<td>t</td>
<td style="text-align:right">每个用户使用优惠券的总次数</td>
</tr>
<tr>
<td>t1</td>
<td style="text-align:right">每个用户使用不同优惠券的次数</td>
</tr>
<tr>
<td>t2</td>
<td style="text-align:right">每个用户使用某张优惠券（使用次数大于1次）的首次和末次使用时间</td>
</tr>
<tr>
<td>t3</td>
<td style="text-align:right">每个用户用优惠券date；本优惠券首、末次间隔；本优惠券首/末次使用date</td>
</tr>
<tr>
<td>t4</td>
<td style="text-align:right">每个用户每天使用优惠券的次数</td>
</tr>
<tr>
<td>t5</td>
<td style="text-align:right">每个用户每天使用每张优惠券的次数</td>
</tr>
<tr>
<td>t6</td>
<td style="text-align:right">用户使用每张优惠券的date，不同date用冒号分隔</td>
</tr>
<tr>
<td>t7</td>
<td style="text-align:right">用户使用每张券的时间，以及和前、后一张券的时间间隔</td>
</tr>
</tbody>
</table>
<p>文件名：data/other_feature3.csv</p>
<p>格式：user_id,coupon_id,this_month_user_receive_same_coupon_count,this_month_user_receive_all_coupon_count,date_received,this_month_user_receive_same_coupon_lastone,this_month_user_receive_same_coupon_firstone,this_day_user_receive_all_coupon_count,this_day_user_receive_same_coupon_count,day_gap_before,day_gap_after</p>
<p>解释：用户id,优惠券id,本月用户使用本券次数，本月用户使用所有券次数，使用时间，本月用户使用本券末次时间、首次时间，本日用户用券总数，本日用户用本券总数，上次用本券间隔，下次用本券间隔</p>
<h4 id="源码分析-2">4.2.1.1. 源码分析</h4><p>t:计算每个用户使用优惠券的总次数：</p>
<pre><code>#将测试集dataset3的userid存在t中
t = dataset3[[&apos;user_id&apos;]]
#给t添加一个列，列名是this_month_user_receive_all_coupon_count，值都是1
t[&apos;this_month_user_receive_all_coupon_count&apos;] = 1
#按照user_id分组，将user_id重复的项目的this_month_user_receive_all_coupon_count相加，然后进行reset_index
#其实就是算出每个用户使用优惠券的总次数
t = t.groupby(&apos;user_id&apos;).agg(&apos;sum&apos;).reset_index()
</code></pre><p>t1:统计每个用户，使用不同优惠券的次数：</p>
<pre><code>t1 = dataset3[[&apos;user_id&apos;,&apos;coupon_id&apos;]]
t1[&apos;this_month_user_receive_same_coupon_count&apos;] = 1
#按照user_id和coupon_id进行分组
#统计每个用户，使用不同优惠券的次数
t1 = t1.groupby([&apos;user_id&apos;,&apos;coupon_id&apos;]).agg(&apos;sum&apos;).reset_index()
</code></pre><p>t2:找出每个人，消费每个券的时间，并用冒号分隔例如：<img src="http://om1bxijvl.bkt.clouddn.com/2017-03-09-20-10-27.png" alt=""> </p>
<pre><code>t2 = dataset3[[&apos;user_id&apos;,&apos;coupon_id&apos;,&apos;date_received&apos;]]
t2.date_received = t2.date_received.astype(&apos;str&apos;)
# 按照user_id&apos;,&apos;coupon_id排序后，提出来date_received，进行agg运算
# agg运算：用冒号连接起来
t2 = t2.groupby([&apos;user_id&apos;,&apos;coupon_id&apos;])[&apos;date_received&apos;].agg(lambda x:&apos;:&apos;.join(x)).reset_index()
</code></pre><p>t2:每个用户使用某张优惠券（使用次数大于1次）的首次和末次使用时间</p>
<pre><code>#apply会返回每个优惠券的使用次数
t2[&apos;receive_number&apos;] = t2.date_received.apply(lambda s:len(s.split(&apos;:&apos;)))
#筛出使用次数大于1次的数据
t2 = t2[t2.receive_number&gt;1]
#对max_date_received赋值为最近一次的使用时间
t2[&apos;max_date_received&apos;] = t2.date_received.apply(lambda s:max([int(d) for d in s.split(&apos;:&apos;)]))
#对min_date_received赋值为最早一次的使用时间
t2[&apos;min_date_received&apos;] = t2.date_received.apply(lambda s:min([int(d) for d in s.split(&apos;:&apos;)]))
# 重新定义t2为以下项目
t2 = t2[[&apos;user_id&apos;,&apos;coupon_id&apos;,&apos;max_date_received&apos;,&apos;min_date_received&apos;]]
</code></pre><p>t3:每个用户使用优惠券的时间、本次优惠券与首次使用的间隔、末次使用的间隔</p>
<pre><code>t3 = dataset3[[&apos;user_id&apos;,&apos;coupon_id&apos;,&apos;date_received&apos;]]
#merge，将两个数据集合并
#将t2和t3在[&apos;user_id&apos;,&apos;coupon_id&apos;]上进行左帧合并，即根据t3合并t2的user_id&apos;,&apos;coupon_id
#t2[[&apos;user_id&apos;,&apos;coupon_id&apos;,&apos;max_date_received&apos;,&apos;min_date_received&apos;]]
#t3[[&apos;user_id&apos;,&apos;coupon_id&apos;,&apos;date_received&apos;]]
#因此合并方式为：找到每个用户每张优惠券的消费时间和对应券的max_date_received与min_date_received
t3 = pd.merge(t3,t2,on=[&apos;user_id&apos;,&apos;coupon_id&apos;],how=&apos;left&apos;)
#t3的this_month_user_receive_same_coupon_lastone项目设置为:此用户消费本张优惠券与最近一次消费本张优惠券的间隔

t3 = t3.apply(pd.to_numeric, args=(&apos;coerce&apos;,))
t3[&apos;this_month_user_receive_same_coupon_lastone&apos;] = t3.max_date_received - t3.date_received
#此用户消费本张优惠券与第一次消费本张优惠券的间隔
t3[&apos;this_month_user_receive_same_coupon_firstone&apos;] = t3.date_received - t3.min_date_received
</code></pre><p>上面跑到<code>t3[&#39;this_month_user_receive_same_coupon_lastone&#39;] = t3.max_date_received - t3.date_received</code>的时候会出现<code>TypeError: unsupported operand type(s) for -: &#39;float&#39; and &#39;str&#39;</code>.</p>
<p>在执行这句话之前，我们看到<code>t3.date_received</code>的类型为<img src="http://om1bxijvl.bkt.clouddn.com/2017-03-09-22-39-21.png" alt=""> </p>
<p>因此我们需要将数据类型先转换为float。在网上查到本方法对本代码有效（暂不知原因）。<a href="http://stackoverflow.com/questions/14450020/unsupported-operand-in-pandas-dataframe-operation" target="_blank" rel="external">参考文献</a></p>
<p><code>t3 = t3.apply(pd.anumeric, args=(&#39;coerce&#39;,))</code></p>
<p>把这句话加上后，我们看到<img src="http://om1bxijvl.bkt.clouddn.com/2017-03-09-22-43-02.png" alt=""> </p>
<p>定义函数is_firstlastone判断优惠券是否是末次使用</p>
<pre><code>def is_firstlastone(x):
    if x==0:
        return 1
    elif x&gt;0:
        return 0
    else:
        return -1 #those only receive once
</code></pre><p>t3:加上两个数据，…</p>
<pre><code>t3.this_month_user_receive_same_coupon_lastone = t3.this_month_user_receive_same_coupon_lastone.apply(is_firstlastone)
t3.this_month_user_receive_same_coupon_firstone = t3.this_month_user_receive_same_coupon_firstone.apply(is_firstlastone)
t3 = t3[[&apos;user_id&apos;,&apos;coupon_id&apos;,&apos;date_received&apos;,&apos;this_month_user_receive_same_coupon_lastone&apos;,&apos;this_month_user_receive_same_coupon_firstone&apos;]]
</code></pre><p>后面套路差不多，此处不再继续分析。主要结论已经总结在上表中。</p>
<h1 id="附录">5. 附录</h1><h2 id="查看dataFrame类型的内容">5.1. 查看dataFrame类型的内容</h2><p>见pandas 文档之 10 minutes to pandas — viewing data</p>
<p>用t.values,t.columns</p>
<h2 id="lambda-functions">5.2. lambda functions</h2><p>源代码中有一行<code>t2.groupby([&#39;user_id&#39;,&#39;coupon_id&#39;])[&#39;date_received&#39;].agg(lambda x:&#39;:&#39;.join(x)).reset_index()</code></p>
<p><a href="http://www.diveintopython.net/power_of_introspection/lambda_functions.html" target="_blank" rel="external">官方文档</a></p>
<p><code>lambda functions</code>是python的一个function.<br>用例：</p>
<pre><code>#函数f(x)
&gt;&gt;&gt; def f(x):
...     return x*2
...
&gt;&gt;&gt; f(3) #输入x=3     
6 #输出6

#f(x)等价于：
&gt;&gt;&gt; g = lambda x: x*2  1
&gt;&gt;&gt; g(3)
6
#f(x)还等价于：
&gt;&gt;&gt; (lambda x: x*2)(3) 2
6
</code></pre><p>作者代码中，有一行<br><code>lambda x:&#39;:&#39;.join(x)</code>即将前后叠加,用<code>:</code>连接<br><code>t2.groupby([&#39;user_id&#39;,&#39;coupon_id&#39;])[&#39;date_received&#39;].agg(lambda x:&#39;:&#39;.join(x)).reset_index()</code>意思是将数据集先按照user_id’,’coupon_id排序，然后对date_received进行用:连接一起来</p>
<p>例如，输入：</p>
<pre><code>df = pd.DataFrame({&apos;A&apos; : [&apos;foo&apos;, &apos;bar&apos;, &apos;foo&apos;, &apos;bar&apos;,
                            &apos;foo&apos;, &apos;bar&apos;, &apos;foo&apos;, &apos;foo&apos;],
                            &apos;B&apos; : [&apos;one&apos;, &apos;one&apos;, &apos;two&apos;, &apos;three&apos;,
                             &apos;two&apos;, &apos;two&apos;, &apos;one&apos;, &apos;three&apos;],
                            &apos;C&apos; : np.random.randn(8),
                            &apos;D&apos; : np.random.randn(8)})
</code></pre><table>
<thead>
<tr>
<th>i</th>
<th style="text-align:right">A</th>
<th style="text-align:right">B</th>
<th style="text-align:right">C</th>
<th style="text-align:right">D</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td style="text-align:right">foo</td>
<td style="text-align:right">one</td>
<td style="text-align:right">0.754147</td>
<td style="text-align:right">0.912176</td>
</tr>
<tr>
<td>1</td>
<td style="text-align:right">bar</td>
<td style="text-align:right">one</td>
<td style="text-align:right">1.414635</td>
<td style="text-align:right">-0.760638</td>
</tr>
<tr>
<td>2</td>
<td style="text-align:right">foo</td>
<td style="text-align:right">two</td>
<td style="text-align:right">-0.142930</td>
<td style="text-align:right">-1.290766</td>
</tr>
<tr>
<td>3</td>
<td style="text-align:right">bar</td>
<td style="text-align:right">three</td>
<td style="text-align:right">1.196999</td>
<td style="text-align:right">1.647513</td>
</tr>
<tr>
<td>4</td>
<td style="text-align:right">foo</td>
<td style="text-align:right">two</td>
<td style="text-align:right">-0.261663</td>
<td style="text-align:right">1.284779</td>
</tr>
<tr>
<td>5</td>
<td style="text-align:right">bar</td>
<td style="text-align:right">two</td>
<td style="text-align:right">1.622070</td>
<td style="text-align:right">1.685648</td>
</tr>
<tr>
<td>6</td>
<td style="text-align:right">foo</td>
<td style="text-align:right">one</td>
<td style="text-align:right">1.478855</td>
<td style="text-align:right">-0.229636</td>
</tr>
</tbody>
</table>
<pre><code>df3 = df.groupby([&apos;A&apos;])[&apos;B&apos;].agg(lambda x:&apos;:&apos;.join(x)).reset_index()
</code></pre><p>输出：</p>
<table>
<thead>
<tr>
<th>i</th>
<th style="text-align:right">A</th>
<th style="text-align:right">B</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td style="text-align:right">bar</td>
<td style="text-align:right"><code>one:three:two</code></td>
</tr>
<tr>
<td>1</td>
<td style="text-align:right">foo</td>
<td style="text-align:right"><code>one:two:two:one:three</code></td>
</tr>
</tbody>
</table>
<h2 id="pandas的merge的how参数">5.3. pandas的merge的how参数</h2><p>原代码出现了<code>t3 = pd.merge(t3,t2,on=[&#39;user_id&#39;,&#39;coupon_id&#39;],how=&#39;left&#39;)</code></p>
<p>how参数主要决定了哪一个keys会被包含在结果表中。它的值有四种可能性：<code>left,right,outer,inner</code>。我们主要看<code>left和right</code></p>
<p>how=’left’:遍历left表，找与right一样的，依次放入行。 如果没有，则设为NAN</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-09-20-34-44.png" alt=""> </p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-09-20-33-13.png" alt=""> </p>
<p>因此<code>t3 = pd.merge(t3,t2,on=[&#39;user_id&#39;,&#39;coupon_id&#39;],how=&#39;left&#39;)</code>的意思是：</p>
<p>根据t3合并t2的user_id’,’coupon_id</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/o2o优惠券使用预测/">o2o优惠券使用预测</a>
</div>


</div>




<div class="comments-count">
	
	  	<span></span>
		<a href="/2017/03/08/O2O优惠券预测——对第一名的思路源码分析（一）/#comments" class="ds-thread-count comments-count-link" data-thread-key="2017/03/08/O2O优惠券预测——对第一名的思路源码分析（一）/" data-count-type="comments">&nbsp;</a>
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/03/06/梯度提升决策树 AdaBoost DecisonTree/" title="梯度提升决策树 AdaBoost DecisonTree" itemprop="url">梯度提升决策树 AdaBoost DecisonTree</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="jiayi797" target="_blank" itemprop="author">jiayi797</a>
		
  <p class="article-time">
    <time datetime="2017-03-06T11:28:33.000Z" itemprop="datePublished"> 发表于 2017-03-06</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>上一节中介绍了《随机森林算法》，该算法使用bagging的方式作出一些决策树来，同时在决策树的学习过程中加入了更多的随机因素。该模型可以自动做到验证过程同时还可以进行特征选择。 </p>
<p>本节，我们结合<code>AdaBoost+决策树</code>算法。</p>
<h1 id="AdaBoost决策树算法引入">1. AdaBoost决策树算法引入</h1><p>在AdaBoost中每一轮迭代，都会给数据更新一个权重，利用这个权重，我们学习得到一个g，在这里我们得到一个决策树，最终利用线性组合的方式得到多个决策树组成的G。</p>
<p>=======================================<br><strong>AdaBoost-DTree(DD)</strong><br>对于t=1,2,…,T，循环执行：</p>
<ul>
<li>更新数据的权重$u(t)$；</li>
<li>通过决策树算法$DTree(D,u(t))$得到$g_t$；</li>
<li>计算$g_t$的投票权重$α_t$。</li>
</ul>
<p>返回$G=LinearHypo({(g_t,α_t)})$。</p>
<p>========================================</p>
<p><strong>问题</strong>：如何要在决策树中，加入权重<code>ut</code></p>
<p><strong>解决方案</strong>有两种：</p>
<ul>
<li>一种是通过算法加权，在计算Ein的地方嵌入权重计算，比如AdaBoost采用的最小化加权误差；</li>
<li>另一种方法是将算法当成黑盒不变更，通过数据集加权，根据权重在bootstrap时“复制”数据，也就是加权的重采样。</li>
</ul>
<p>AdaBoost决策树通常用后一种，即：$AdaBoost+sampling∝u^{(t)}+DTree(D_t) $</p>
<h1 id="加权的决策树算法-Weighted-Decision-Tree-Algorithm">2. 加权的决策树算法(Weighted Decision Tree Algorithm)</h1><p> 之前含有权重的算法中，我们在误差估计中加入了权重<code>u</code>：</p>
<p> <img src="http://om1bxijvl.bkt.clouddn.com/2017-03-07-09-46-47.png" alt=""> </p>
<p>为了对决策树中加入权重，且不改变原算法的健壮性，我们设法对输入的<code>数据</code>进行<code>权重加成</code>。而权重等效于数据的重复次数。根据这种方式得到一组新的数据，那么这组新的数据中的比例大概就是和权重的比例呈正比的，也就是说它能够表达权重对于数据的意义。</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-07-09-54-14.png" alt=""> </p>
<p>在AdaBoost-DTree中，为了简单起见，我们不去改变AdaBoost的框架，也不去修改决策树的内部细节，而只是通过基于权重的训练数据的采样来实现。</p>
<p>即如下图所示的：AdaBoost提升决策树=AdaBoost提升+关于权重u的数据抽样+决策树</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-07-09-54-50.png" alt=""> </p>
<h2 id="弱决策树算法">2.1. 弱决策树算法</h2><p>在AdaBoost算法中，我们<strong>通过错误率<code>εt</code>来计算单个的g的权重αt</strong>，那么如果我们使用决策树作为g的时候，g是一个完全长成的树，该树对整个数据集进行细致的切分导致Ein=0，那么这使得εt=0，但计算得到的权重αt会变成无限大。</p>
<p>其意义是，如果使用一个能力很强的树作为g的话，那么该算法会赋予该树无限大的权重或票数，最终得到了一棵“独裁”的树（因为AdaBoost的哲学意义是庶民政治，就是集中多方的意见，及时有的意见可能是错误的），违背了AdaBoost的宗旨。</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-07-09-59-58.png" alt=""> </p>
<p>上面的问题出在使用了所有的数据和让树完全长成这两方面。针对这两个问题，我们要通过<code>剪枝</code>和<code>部分训练数据</code>得到一个弱一点的树。<br>所以实际上，AdaBoost-DTree是通过sampling的方式得到部分训练数据，通过剪枝的方式限制树的高度，得到弱一点的决策树。</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-07-10-19-02.png" alt=""> </p>
<p>下面介绍最弱的决策树。</p>
<h2 id="决策桩，AdaBoost-Stump">2.2. 决策桩，AdaBoost-Stump</h2><p>什么样是树才是弱决策树呢？<br>我们这里限制这棵树只有一层（即它仅基于单个特征来做决策），即决策桩(Decision Stump)。这样我们需要让CART树的不纯度(impurity)尽可能低，学习一个决策桩。</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-07-10-21-14.png" alt=""> </p>
<p>所以，使用决策桩作为弱分类器的AdaBoost称为AdaBoost-Stump，它是一种特殊的AdaBoost-DTree。</p>
<h2 id="决策桩的实现">2.3. 决策桩的实现</h2><p>本节主要参考《机器学习实战》p120</p>
<h3 id="实验数据adaboost-py">2.3.1. 实验数据adaboost.py</h3><pre><code>from numpy import *
def loadSimpData():
    dataMat = matrix([[1.,2.1],[2.,1.1],[1.3,1.],[1.,1.],[2.,1.]])
    classLabels = [1.0,1.0,-1.0,-1.0,1.0]
    return dataMat,classLabels
</code></pre><h3 id="二分类的决策桩实现stump-py">2.3.2. 二分类的决策桩实现stump.py</h3><p>先导入数据</p>
<pre><code>import adaboost
dataMat,classLabels = adaboost.loadSimpData()
</code></pre><p>建立一个<code>buidStump()</code>函数，根据数据集，建立最佳单层决策树（只需要选择一个最好的特征即可）</p>
<pre><code>def buildStump(dataArr,classLabels,D):
    dataMatrix = mat(dataArr)
    labelMat = mat(classLabels).T # T是做转置
</code></pre><p><code>dataMatrix</code>形式为<img src="http://om1bxijvl.bkt.clouddn.com/2017-03-16-16-29-28.png" alt=""><br><code>labelMat</code>形式为<img src="http://om1bxijvl.bkt.clouddn.com/2017-03-16-16-29-39.png" alt=""> </p>
<p>先令一些变量，之后解释。</p>
<pre><code>m,n = shape(dataMatrix)
numSteps = 10.0#步长
bestStup = {}#最佳桩
bestClasEst = mat(zeros((m,1)))#最佳分类est
minError = inf
</code></pre><p>接下来需要对每个特征计算出一个阈值<code>threshVal</code>，根据阈值二分类。</p>
<pre><code>for i in range(n): # 遍历特征个数
    #为了确定threshVal，我们从本特征下的最小值到最大值分10 step进行依次测试
    rangeMin = dataMatrix[:,i].min();rangeMax = dataMatrix[:,i].max();
    stepSize = (rangeMax - rangeMin)/numSteps
    #下面对每个threshVal可能的值进行依次测试
    for j in range(-1,int(numSteps)+1):
        #然后应该开始比较大于阈值和小于阈值怎么怎么滴，为了增加代码的复用性，此处用一个循环来在大于和小于之间切换不等式
        for inequal in [&apos;lt&apos;,&apos;gt&apos;]:#lt=小于等于，gt=大于
            threshVal = (rangeMin + float(j)*stepSize)
            # 开始测试这个特征下这个阈值的二分类器好不好用
            predictedVals = stumpClassify(dataMatrix,i,threshVal,inequal)
            #计算本次分类的err
            errArr = mat(ones((m,1)))
            errArr[predictedVals==labelMat]=0
            #基于权重向量D计算权重
            weightedError = D.T*errArr
            if weightedError &lt; minError :
                minError = weightedError
                bestClasEst = predictedVals.copy()
                bestStump[&apos;dim&apos;] = i
                bestStump[&apos;thresh&apos;] = threshVal
                bestStump[&apos;ineq&apos;] = inequal
</code></pre><p>最后，返回最佳的决策桩，和误差</p>
<pre><code>return bestStump,minError,bestClasEst
</code></pre><h1 id="求解AdaBoost决策树">3. 求解AdaBoost决策树</h1><h2 id="AdaBoost的权重与投票分数的关系">3.1. AdaBoost的权重与投票分数的关系</h2><p>回顾AdaBoost算法：</p>
<p>从权重<code>ut</code>，通过<code>◆t</code>对<code>u(t+1)</code>进行修正，而两个公式可以合成为：<br><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-07-14-35-34.png" alt=""> </p>
<p>如下图，接着我们将<code>u(t+1)</code>展开(表达为<code>u(1)乘以一坨</code>)，最终可以变成连加；<br>我们发现图中橘色部分<code>∑αt·gt(xn)</code>是G(x)的分数！它现在出现在Adaboost的权重表达式中；<br>我们称橘色<code>∑αt·gt(xn)</code>为<strong>投票分数(voting score)</strong>：<br><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-07-14-38-40.png" alt=""> </p>
<p><strong>结论</strong>：AdaBoost里面每一个数据的权重，和<code>exp(-yn( 投票分数 on xn))</code>呈正比。即：<img src="http://om1bxijvl.bkt.clouddn.com/2017-03-07-14-41-04.png" alt=""> </p>
<h2 id="投票分数-Voting-Score-和间隔-Margin-的关系">3.2. 投票分数(Voting Score)和间隔(Margin)的关系</h2><p>线性混合(linear blending)等价于将假设看做是特征转换的线性模型：</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-07-19-13-20.png" alt=""> </p>
<p>其中<code>αt·gt(xn)</code>如果换作是<code>wT·φ(xn)</code>可能就更清楚了，这与下面给出的在SVM中的margin表达式对比，我们可以明白投票分数<code>∑αt·gt(xn)</code>的物理意义，即可以看做是没有正规化的边界(margin)。</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-07-19-20-39.png" alt=""> </p>
<p>所以，<code>yn·(voting score)</code>是有符号的、没有正规化的边界距离，从这个角度来说，我们希望<code>yn·(voting score)</code>越大越好，因为这样的泛化能力越强。于是，<code>exp(-yn·(voting score))</code>越小越好，那么<code>un(T+1)</code>越小越好。</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-07-19-21-01.png" alt=""> </p>
<p><strong>结论</strong>：AdaBoost在迭代过程中，是让<code>∑un(t)</code>越来越小的过程，在这个过程中，逐渐达到SVM中最大分类间隔的效果。</p>
<h2 id="AdaBoost误差函数">3.3. AdaBoost误差函数</h2><p>上面解释到了，AdaBoost在迭代学习的过程，就是希望让<code>∑un(t)</code>越来越小的过程，那么我们<strong>新的目标</strong>就是最佳化权重和<code>∑un(T+1)</code>：</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-07-19-58-44.png" alt=""> </p>
<p>我们可以画出<code>0/1错误</code>和<code>AdaBoost误差函数err(s,y) = exp(-ys)</code>的函数曲线，我们发现AdaBoost的误差函数（称为exponential error measure）实际上也是0/1错误函数的上限函数，于是，<strong>我们可以通过最小化该函数来起到最佳化的效果</strong>。</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-07-19-59-15.png" alt=""> </p>
<h2 id="AdaBoost误差函数的梯度下降求解">3.4. AdaBoost误差函数的梯度下降求解</h2><p>本节目的————最小化AdaBoost的误差函数<code>Ein</code>：</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-07-19-58-44.png" alt=""> </p>
<p>这个任务比较麻烦，因为是Σ套着exp再套着Σ，因此需要一些前人的智慧了。</p>
<p>我们可以将<code>Ein</code>函数在所在点的附近做泰勒展开，我们就可以发现在该点的附近可以被梯度所描述，我们希望求一个最好的方向（最大梯度相反的方向），然后在该方向上走一小步，这样我们就可以做到比现在的函数效果好一点点，依次进行梯度下降，最终达到最小化误差函数的效果。</p>
<p>原始的梯度下降法：</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-07-20-02-45.png" alt=""> </p>
<p>为了模仿梯度下降的方法，假设前面已经AdaBoost完t-1轮了，现在要求的是一个函数gt(x)（或者称为h(x)）。</p>
<p>在第t轮，我们沿着函数h(x)的方向走$η$的步长，可以使得目标函数迅速往min的方向走。如下：现在我们把<code>函数gt</code>当做向量，希望去找到这个<code>gt</code>（这里函数方向gt和上面介绍的最大梯度的方向向量没有什么差别，只是表示方式有所不同而已）。</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-07-20-03-24.png" alt=""> </p>
<p>我们解释一下上面的公式：</p>
<ul>
<li>(1、2行)由于前面已经执行完了<code>t-1</code>轮，因此可以把式子化简一下，把一些项目合并成<code>ut</code>的函数形式</li>
<li>(3行左) 将<code>exp(-y·η·h(xn))</code>在原点xn=0点的泰勒展开，进一步化简得到得到<code>(1-yn·η·h(xn))</code>；（这里为什么要用0这个位置的taylor展开呢，可以理解成h(x)只是沿着原来的Σ1,t-1(alphat*g’(xn)这个函数，挪动的了一小步；这一小步，就意味着变化很小，变化很小甚至接近0，因此就可以在0点taylor展开。不晓得这种理解是否正确，意会吧）</li>
<li>(3行右) 然后拆成两部分<code>∑un(t)</code>和<code>η·∑un(t)·yn·h(xn)</code>，第一部分是Ein，第二部分就是要最小化的目标。<br><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-07-20-04-07.png" alt=""> </li>
</ul>
<p>到此，我们利用前人的智慧已经把目标函数给大大简化了，下面需要要求的东西有俩：</p>
<p>1）<code>h(x)</code>是啥？</p>
<p>2）<code>$η$</code>是啥？</p>
<h3 id="求h-x">3.4.1. 求h(x)</h3><p>我们对<code>∑un(t)·yn·h(xn)</code>整理一下，对于二元分类情形，我们把<code>yn</code>和<code>h(xn)</code>是否同号进行分别讨论：</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-07-20-04-29.png" alt=""> </p>
<p>上面的公式中，我们特意将<code>∑un(t)·yn·h(xn)</code>拆成<code>-∑un(t)</code>和<code>Ein(h)</code>的形式，这里最小化<code>Ein</code>对应于AdaBoost中的A（弱学习算法），好的弱学习算法就是对应于梯度下降的函数方向。</p>
<p><strong>结论</strong>：在AdaBoost的过程中，算法A就是good gt了！</p>
<h3 id="求最佳化步长-η">3.4.2. 求最佳化步长$η$</h3><p>我们要最小化Eada，需要找到好的函数方向gt，但是得打这个gt的代价有些大，梯度下降的过程中，每走一小步，就需要计算得到一个gt。如果转换一下思路，我们现在已经确定了好的gt，我们希望快速找到梯度下降的最低点，那么我们需要找到一个合适的最大步长η。<br><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-07-20-05-39.png" alt=""> </p>
<p>我们这里使用贪心算法来得到最大步长η，称为steepest decent for optimization。</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-07-20-05-54.png" alt=""> </p>
<p>让Eada对η求偏微分，得到最陡时候的ηt，我们发现这时ηt等于AdaBoost的αt。所以在AdaBoost中αt是在偷偷地做最佳化的工作。</p>
<p>核心在于EADA是怎么变成可对$η$求导的形式的：</p>
<p>EADA = u1t<em>exp(-$η$) + u2t</em>exp($η$)…</p>
<p>EADA1 = u1t<em>exp(-$η$) + ut2t</em>0 … （EADA1只考虑exp(-$η$)的项，其余的补上0）</p>
<p>EADA2 = u1t<em>0 + u2t </em> exp($η$) …（EADA2只考虑exp(+$η$)的项，其余的补上0）</p>
<p>则，EADA = EADA1 + EADA1 = (Σunt) <em> ( (1-epson)exp(-$η$) + epson</em>exp($η$) )</p>
<p>随后的求导步骤就是很自然的了，因此就验证了之前的结论，$η$t = sqrt( (1-epsont)/epsont) )就是最优的。前一次课直接给出了这个结论，并没有说为什么，这次算是给出了一个相对理论些的推导。</p>
<p><strong>结论</strong>：通过求解<img src="http://om1bxijvl.bkt.clouddn.com/2017-03-15-13-25-13.png" alt=""><br>，我们得到最佳的<img src="http://om1bxijvl.bkt.clouddn.com/2017-03-15-13-25-32.png" alt=""> </p>
<h3 id="小结">3.4.3. 小结</h3><p>在第二小节中，我们从另外一个角度介绍了AdaBoost算法，它其实是steepest gradient decent。</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-07-20-06-19.png" alt=""> </p>
<p>上面的式子很清楚了，我们将AdaBoost的误差函数看做是指数误差函数，AdaBoost就是在这个函数上一步一步做最佳化，每一步求得一个h，并将该h当做是gt，决定这个gt上面要走多长的距离ηt，最终得到这个gt的票数αt。</p>
<h1 id="AdaBoost决策树总结">4. AdaBoost决策树总结</h1><ol>
<li>AdaBoost本次的u(t+1)与<code>exp(-yn( 投票分数 on xn))</code>成正比</li>
<li>AdaBoost在迭代过程中，是让<code>∑un(t)</code>越来越小的过程，在这个过程中，逐渐达到SVM中最大分类间隔的效果</li>
<li>上目标与最小化误差函数<code>err(s,y) = exp(-ys)</code>等价</li>
<li>要使得<code>err(s,y)</code>最小，就需要求得<code>h(x)</code>和<code>η</code></li>
</ol>
<h1 id="参考文献">5. 参考文献</h1><ol>
<li><a href="http://qianjiye.de/2015/01/gradient-boosted-decision-tree" target="_blank" rel="external">梯度提升决策树</a></li>
<li><a href="http://www.cnblogs.com/xbf9xbf/p/4706150.html" target="_blank" rel="external">【Gradient Boosted Decision Tree】林轩田机器学习技术</a></li>
<li>《机器学习实战》</li>
</ol>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
</div>


</div>




<div class="comments-count">
	
	  	<span></span>
		<a href="/2017/03/06/梯度提升决策树 AdaBoost DecisonTree/#comments" class="ds-thread-count comments-count-link" data-thread-key="2017/03/06/梯度提升决策树 AdaBoost DecisonTree/" data-count-type="comments">&nbsp;</a>
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/03/05/自适应提升 AdaBoost/" title="提升方法 AdaBoost" itemprop="url">提升方法 AdaBoost</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="jiayi797" target="_blank" itemprop="author">jiayi797</a>
		
  <p class="article-time">
    <time datetime="2017-03-05T09:31:52.000Z" itemprop="datePublished"> 发表于 2017-03-05</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>提升（boosting）：从弱学习算法（正确率低）出发，反复学习，得到一系列弱分类器（基本分类器），然后组合这些弱分类器，构成一个强分类器。</p>
<p>提升（boosting）方法需要解决的问题：</p>
<ul>
<li>如何改变训练数据的权值或概率分布————提高被前一轮错误分类样本的权值，降低被正确分类样本的权值。</li>
<li>如何将弱分类器合成一个强分类器————加权多数表决：加大误差小的分类器的权值，减小误差大的分类器的权值。</li>
</ul>
<h1 id="AdaBoost算法">1. AdaBoost算法</h1><p>假设给定一个二分类训练数据集$T=\{(x_1,y_1),(x_2,y_2),…,(x_N,y_N)\}$</p>
<p>其中，每个样本点由<code>实例+标记</code>组成</p>
<p>实例：$x_i\in X \subseteq R^n $</p>
<p>标记：$y_i \in Y={-1,+1}$</p>
<p><code>AdaBoost</code>利用以下算法，从训练数据中学习一系列弱分类器或基本分类器，并将这些弱分类器线性组合成一个强分类器</p>
<p><strong>输入</strong>:</p>
<ul>
<li>数据集$T=\{(x_1,y_1),(x_2,y_2),…,(x_N,y_N)\}$</li>
<li>弱学习算法</li>
</ul>
<p><strong>输出</strong>：最终分类器$G(x)$</p>
<p><strong>步骤</strong>：</p>
<ol>
<li>初始化训练数据的权值(每个都设为1/N)：</li>
</ol>
<p>$$D<em>1=(w</em>{11},w<em>{1i},…,w</em>{1N}),w_{1i}=\frac{1}{N},i=1,2,…,N$$</p>
<ol>
<li>对m=1,2,…,M:</li>
</ol>
<ul>
<li>使用带权值$D_m$的训练集学习，得到基本分类器$G_m(x):X\rightarrow \{-1,+1\}$</li>
<li>计算$G_m(x)$在训练集上的分类误差率：$$e_m=P(G_m(x_i)\neq y<em>i)=\sum</em>{n=1}^N w_{mi}I(G_m(x_i)\neq y_i)$$</li>
</ul>
<ol>
<li>计算$G_m(x)$的系数：$$\alpha_m=\frac{1}{2}log\frac{1-e_m}{e_m}$$</li>
<li>更新训练集权值：</li>
</ol>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-06-22-49-14.png" alt=""> </p>
<ol>
<li>构建基本分类器的线性组合：</li>
</ol>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-06-22-49-54.png" alt=""> </p>
<h1 id="AdaBoost算法推导">2. AdaBoost算法推导</h1><h2 id="Boot-strapping">2.1. Boot strapping</h2><p>Boot strapping，拔靴法：利用有限的样本资料经由<strong>多次重复抽样</strong>，重新建立起足以代表母体样本分布之新样本。</p>
<p>多次之后，得到一个非线性的结果（黑色线）<br><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-04-17-37-57.png" alt=""> </p>
<h2 id="基本算法引入权重">2.2. 基本算法引入权重</h2><p>已知：一笔数据$D=\{(x_1,y_1),(x_2,y_2),(x_3,y_3),(x_4,y_4)\}$<br>根据<code>D</code>算出来的输入误差Ein为：<br><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-04-19-29-41.png" alt=""> </p>
<p>通过Boot strapping，得到新的一笔数据$D_t=\{(x_1,y_1),(x_1,y_1),(x_2,y_2),(x_4,y_4)\}$<br>对应地，根据<code>Dt</code>算出来的Ein为：<br>（增加一个权重u即可）<br><code>u1=2,u2=1,u3=0,u4=1</code><br><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-04-19-31-04.png" alt=""> </p>
<p><strong>结论：每一个bootstrapping得到了一个权重<code>u</code></strong></p>
<h2 id="优化权重u">2.3. 优化权重u</h2><h3 id="优化原理">2.3.1. 优化原理</h3><ul>
<li>每一个bootstrapping得到了一个权重`u。</li>
<li>为了综合得到更好的g,则需要抽取的数据集得到的g尽量地不同。</li>
<li>改变<code>u</code>，使得<code>g</code>差异更大，才会更好地改进最终结果</li>
</ul>
<p>得到g差异很大的方法：</p>
<ul>
<li>第一轮$u_n^t$时，得到$g_t$<img src="http://om1bxijvl.bkt.clouddn.com/2017-03-04-19-39-48.png" alt=""> </li>
<li>第二轮，选择一个 在$g_t$ 表现不好的 $u<em>n^{t+1}$  ，得到 $g</em>{t+1}$ <img src="http://om1bxijvl.bkt.clouddn.com/2017-03-04-19-41-02.png" alt=""><br>– 表现不好的定义：<br>— 将$u_n^{t+1}$作用在$g_t$上，得到一个归一化的错误率<img src="http://om1bxijvl.bkt.clouddn.com/2017-03-04-19-45-01.png" alt=""><br>— 为了简便，定义橙色方块为所有犯错误的$u_n^{t+1}$的累加，绿色圆形为所有正确的$u_n^{t+1}$累加<br>— 即：<img src="http://om1bxijvl.bkt.clouddn.com/2017-03-04-19-47-12.png" alt=""><br>– 表现不好的选择方法：<br>— 将本次正确的$u_n^t$，除以一个错误的比例（缩小正确），赋给$u_n^{t+1}$<br>— 将本次错误的$u_n^t$，乘以一个正确的比例（放大错误），赋给$u_n^{t+1}$<br>— 这样得到的$u_n^{t+1}$的比率就会为2/1<br>— 即：<img src="http://om1bxijvl.bkt.clouddn.com/2017-03-04-19-51-21.png" alt=""> </li>
</ul>
<h3 id="优化权重u的方法————放缩因子">2.3.2. 优化权重u的方法————放缩因子</h3><p>放缩因子-Adaptive Boosting Algorithm<br><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-04-20-02-12.png" alt=""> </p>
<ul>
<li>◆t有更清晰的物理意义，通常情况下εt &lt; 1/2（因为是学习之后的结果，错误率应该小于0.5），</li>
<li>◆t将大于1；</li>
<li>那么，犯错的数据将乘上大于1的数，正确数据将除以大于1的数</li>
<li>使得提升了犯错数据的权重(scale up incorrect)，</li>
<li>降低做对数据的权重(scale down correct)</li>
<li>这样使得更加专注在犯了错的地方，来得到不一样的假设(diverse hypotheses)。</li>
</ul>
<h2 id="Linear-Aggregation（聚集）-合成最终的g">2.4. Linear Aggregation（聚集） - 合成最终的g</h2><p>目标：合成最终的的$G(x)=sign(\sum_{t=1}^T\alpha_t g_t(x)$</p>
<ul>
<li>其中 $\alpha_t$是系数</li>
<li>要求好的$g_t$（错误率低），$\alpha_t$应该大一些</li>
<li>坏的$g_t$（错误率高），$\alpha_t$应该小一些</li>
<li>而◆t与错误率成反比</li>
<li>则可令$\alpha_t=ln(\text{◆t})$</li>
</ul>
<p>算法流程：<br><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-04-20-21-25.png" alt=""> </p>
<p>这里之所以认为αt = ln(◆t)，处于下面的考虑：<br>如果εt = 1/2， 那么◆t = 1，则αt = 0，意思是随机乱猜的情况下（二元分类错误率为0.5），认为是坏的g，则一票不给个，不使用该g<br>如果εt = 0， 那么◆t = ∞，则αt = ∞，意思是正确率为0的情况，给它无限多票数</p>
<h1 id="AdaBoost-自适应优化算法总结">3. AdaBoost 自适应优化算法总结</h1><p>自适应优化算法 = 简单的学习A + 放缩权重 + 合成得到g<br>即：<img src="http://om1bxijvl.bkt.clouddn.com/2017-03-04-20-25-49.png" alt=""> </p>
<p>AdaBoost算法完整流程<br><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-04-20-26-16.png" alt=""> </p>
<h1 id="AdaBoost理论特性">4. AdaBoost理论特性</h1><p>通过之前的VC Bound，来约束测试误差，其中蓝色的部分是模型的复杂度，O(dvc(H))为g的模型复杂度，而O(dvc(H))·T·logT是模型G的复杂度。原作者证明说，可以用O(logN)次迭代可以将Ein(G)做到很小，并且当数据量N足够多的情况下，又可以使得模型复杂度变得很小，从而使得模型复杂度得到控制。最终预测效果Eout也会很好。<br><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-04-20-27-24.png" alt=""><br>AdaBoost的保证是让一个很弱的算法不断变强，最终得到一个很强是算法（Ein=0，Eout is small）。<br><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-04-20-27-48.png" alt=""> </p>
<h1 id="Adaptive-Boosting的实际应用表现">5. Adaptive Boosting的实际应用表现</h1><p>上面的AdaBoost只需要一个很弱的算法就可以使用。<br>一般情况下，可以使用决策桩(Decision Stump)，该模型相当于在某一个维度上的Perceptron模型。<br><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-04-20-28-27.png" alt=""> </p>
<h1 id="聚合（aggregation）模型总结">6. 聚合（aggregation）模型总结</h1><p>aggregation 模型主要应用在将得到的多个预测函数$g_t$聚合在一起，得到更好的$g_t$（即更好的分类器）的方式</p>
<p>聚合方式主要面向两种情况：</p>
<ul>
<li>blending:已经有了一堆$g_t$在手上（可能是已知的，可能是求得的）。</li>
<li>learning：不已知$g_t$，需要通过一定方式求得很多$g_t$</li>
</ul>
<p>learning的分为三种情况</p>
<ul>
<li>把g看做是同等地位，通过投票或者平均的方式将它们合起来，称为Bagging</li>
<li>g是不平等的，有好有坏，一个可行的做法是把g当成是特征的转换，然后丢进线性模型训练就可以了，这称为AdaBoost</li>
<li>如果是不同的条件下，使用不同的g，那么我们仍然可以将g当做是特征转换，接下来使用一个非线性模型来得到最终的模型参数，这就是下文要介绍的决策树算法</li>
</ul>
<table>
<thead>
<tr>
<th>$g_t$类型</th>
<th style="text-align:right">blending</th>
<th style="text-align:center">learning</th>
</tr>
</thead>
<tbody>
<tr>
<td>各$g_t$等权重型（uniform）</td>
<td style="text-align:right">投票方式/平均方式</td>
<td style="text-align:center">Bagging</td>
</tr>
<tr>
<td>$g_t$权重不等型（non-uniform）</td>
<td style="text-align:right">线性聚合</td>
<td style="text-align:center">AdaBoost</td>
</tr>
<tr>
<td>不同情形用不同$g_t$（conditional）</td>
<td style="text-align:right">stacking</td>
<td style="text-align:center">决策树</td>
</tr>
</tbody>
</table>
<h1 id="AdaBoost思路总结">7. AdaBoost思路总结</h1><ul>
<li>一般，数据量过少时，我们无法得到更好的g.</li>
<li>因此我们采取BootStrapping方法，生成多个数据集，得到多个g</li>
<li>最后合成最好的g</li>
</ul>
<h1 id="AdaBoost伪代码">8. AdaBoost伪代码</h1><pre><code>对每次迭代：
    用buildStump()函数找到最佳单层决策树
    将最佳单层决策树加入到单层决策树数组
    计算alpha
    计算新的权重向量D
    更新累积类别估计值
    如果错误率等于0，则退出循环
</code></pre><p>参考文献</p>
<ol>
<li>《机器学习技法》，林轩田</li>
<li><a href="http://blog.csdn.net/JasonDing1354/article/details/46462711" target="_blank" rel="external">Jason Ding，【机器学习基础】自适应提升</a></li>
<li><a href="http://blog.jasonding.top/2015/07/22/Machine%20Learning/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95/" target="_blank" rel="external">Jason Ding，【机器学习基础】决策树算法</a></li>
</ol>
<p>备注：本节是《机器学习技法》第8章+《统计学习方法》第8章笔记</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>




<div class="comments-count">
	
	  	<span></span>
		<a href="/2017/03/05/自适应提升 AdaBoost/#comments" class="ds-thread-count comments-count-link" data-thread-key="2017/03/05/自适应提升 AdaBoost/" data-count-type="comments">&nbsp;</a>
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/03/05/随机森林算法/" title="随机森林算法" itemprop="url">随机森林算法</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="jiayi797" target="_blank" itemprop="author">jiayi797</a>
		
  <p class="article-time">
    <time datetime="2017-03-05T08:49:09.000Z" itemprop="datePublished"> 发表于 2017-03-05</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p><strong>引言</strong></p>
<p>回顾之前学习过的两个算法：</p>
<ul>
<li>Bagging<br>– 简要：通过bootstrapping得到不一样的数据，得到不同的g，对g取平均得到G<br>– 特点：通过投票和平均的方式来降低对不同数据的敏感性（variance的效果）</li>
<li>决策树<br>– 简要：通过递归方式建立子树，最终得到完整的树<br>– 特点：对不同数据较敏感（算法的variance很大）</li>
<li>随机森林<br>– 两者的结合</li>
</ul>
<h1 id="随机森林算法">1. 随机森林算法</h1><p>概述：利用随机的方式将许多决策树组合成一个森林,每个决策树$g_t(t)$在分类的时候投票决定测试样本的最终类别。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。</p>
<p>详细算法：<br><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-05-17-09-01.png" alt=""> </p>
<ul>
<li>左边的总算法是Bagging思想–体现随机性</li>
<li><p>其中为每个$g_t(t)$建树时，是决策树的思想–体现森林</p>
</li>
<li><p>并行计算的可能性：随机森林算法从Bagging过程中可以分配到不同的计算机中进行计算，每台计算机可以独立学习一棵树，不同的树之间没有任何依赖关系。这使得Bagging过程很容易实现并行化。</p>
</li>
</ul>
<h1 id="特征投影（Feature-Project">2. 特征投影（Feature Project)</h1><ul>
<li>原来在Bagging中，我们对数据进行抽取，得到不同的数据集，从而产生不同的$g_t$</li>
<li>在随机森林算法中，除了对数据抽取，也可以在<strong>特征</strong>这一角度抽取</li>
<li>例，如果事先我们有100个特征，现在我们可以抽取10个特征<img src="http://om1bxijvl.bkt.clouddn.com/2017-03-05-20-34-44.png" alt=""> </li>
</ul>
<ul>
<li>得到数据集<img src="http://om1bxijvl.bkt.clouddn.com/2017-03-05-20-34-07.png" alt=""> </li>
<li><p>来训练一棵树，这样的方式我们也可以得到很不一样的树，其对于分类的标准显然也很不一样</p>
</li>
<li><p>这等效于一个特征转换，这个过程中，从100维度到10个维度的转换中，相当于作了低维度的投影(Projection)</p>
</li>
<li><p>一般来说，<img src="http://om1bxijvl.bkt.clouddn.com/2017-03-05-20-34-18.png" alt=""> </p>
</li>
</ul>
<ul>
<li>得到的特征实际上是原始特征的随机子集，这使得生成模型过程中的效率也大大提高了</li>
</ul>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-05-17-17-10.png" alt=""> </p>
<h1 id="特征扩展（feature-Expansion）">3. 特征扩展（feature Expansion）</h1><p>每次随机抽取子空间 <code>等效于</code> 对原来的特征向量左乘一个<strong>投影矩阵</strong>$P$,使得$\Phi(X)=P\cdot x$</p>
<p>更加有能力的特征投影就是不再单一选取单一维度的特征，而是将多个维度的特征进行组合(随机的方向)，得到新的一维的特征，这称为<strong>特征扩展</strong>。</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-05-18-43-19.png" alt=""> </p>
<ul>
<li>将多个方向的特征随机合起来(combination)，即对于投影矩阵$P$的每一个方向$p_i$，不再固定方向（row）。即变为$\Phi_i(X)=P_i^T\cdot x$<br>– 一般情况下，会考虑<code>low-dimensional</code>，即投影过去时，一般每次选取少量维度进行投影。即只有$d’’$的<code>非零项</code>被投影过去</li>
<li>这样的方式，包含了随机抽取（random subspace）的思想</li>
<li>一般来说，每次投影都采用新的不一样的投影</li>
</ul>
<h1 id="随机森林的采样过程">4. 随机森林的采样过程</h1><p>在建立森林的每颗决策树$g_t$的过程中，首先需要随机采样数据点。</p>
<p>不是所有数据点都能被采到。以下介绍OOB点</p>
<h2 id="Out-of-bag（OOB）点">4.1. Out-of-bag（OOB）点</h2><p>OOB点：在bootstrapping过程中，有些数据可能没有被选择，这些数据被称为OOB点。例如下表，对于训练每一个决策树$g_t$，其中用*号标注的就是$g_t$的OOB<br><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-05-19-18-06.png" alt=""> </p>
<h3 id="OOB点个数">4.1.1. OOB点个数</h3><p>假设bootstrapping抽了$N’$次数据，探讨会有多少数据不会被抽到：</p>
<ul>
<li>若$N’=N$，某个数据$(x_n,y_n)$从未被抽到的概率是$(1-\frac{1}{N})^N$<br>$$(1-\frac{1}{N})^N=\frac{1}{\frac{N}{N-1}^N}\approx \frac{1}{e}$$</li>
<li>那么每个决策树$g_t$OOB集合的大小就约为$\frac{1}{e}N\approx 0.3N$</li>
</ul>
<h3 id="OOB用途-验证随机森林的G">4.1.2. OOB用途-验证随机森林的G</h3><p>可以用来做测试集-问题在于————验证<code>g</code>还是<code>G</code>？<br>以数据集$(x_N,y_N)为例$</p>
<ul>
<li>验证$g$的必要性不大</li>
<li>验证$G$不方便</li>
<li>可以用来验证<code>除了g1之外的G</code> = $G_N^-(x)=average(g_2,g_3,g_T)$</li>
<li>总之，用来验证$G$表现是否好的方式：<br>$$E_{oob}(G)=\frac{1}{N}\sum_1^N error(y_n,G_n^-(x_n))$$</li>
</ul>
<h1 id="特征选择（feature-selection）">5. 特征选择（feature selection）</h1><p>目的：自动选择需要的特征，去除冗余、不相关的特征<br>优点：降维，减少复杂度；减少噪声，提高模型泛化能力；物理意义；<br>缺点：计算量大；可能导致过拟合；</p>
<p>下面介绍特征选择的方法。</p>
<h2 id="根据重要性选择（线性的）">5.1. 根据重要性选择（线性的）</h2><ul>
<li>给每个特征算一个权重（分数）</li>
<li>问题：特征选择是线性的，不符合随机森林的非线性特点</li>
</ul>
<h2 id="置换检验（非线性的，Permutation-Test）">5.2. 置换检验（非线性的，Permutation Test）</h2><p>问题：每个特征是有噪音的，由于噪音的存在，导致某些原本很优秀的特征的分数被降低</p>
<p>解决方法：将第i个维度特征的所有数据重新的随机调整位置，然后比较一下原始数据和调整之后的数据表现的差距，来评价这个维度的特征是有多么重要。</p>
<ul>
<li>调整方法1：高斯什么的，但会改变数据原始分布</li>
<li>调整方法2：随机重排，即置换检验。将某一维度的数据随机重排，可以看出来这个维度有多重要。</li>
</ul>
<h2 id="在Out-Of-Bag-Estimate过程中做Permutation-Test">5.3. 在Out-Of-Bag Estimate过程中做Permutation Test</h2><p>在随机森林中可以用OOB代替验证的过程，为了简化Permutation Test带来的重新进行训练的代价，我们在使用OOB Example（bootstrap过程中没有选取的数据）进行验证的过程中做一些修改，即在验证的时候去进行Permutation Test，而非训练时进行。<br>在求Eoob(G)时，我们通过G-(xn)来计算，我们在这里将x(n)修改成x(n,i)，就可以不用对G进行修改了。<br><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-05-20-17-46.png" alt=""><br>在实际应用中，面对非线性的问题时，可以通过随机森林的方法来进行初步的特征选择。</p>
<p> <img src="http://om1bxijvl.bkt.clouddn.com/2017-03-05-20-16-21.png" alt=""> </p>
<p>参考资料：</p>
<ol>
<li><a href="http://database.51cto.com/art/201407/444788.htm" target="_blank" rel="external">机器学习的算法(1):决策树之随机森林</a></li>
<li>机器学习技法</li>
</ol>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
</div>


</div>




<div class="comments-count">
	
	  	<span></span>
		<a href="/2017/03/05/随机森林算法/#comments" class="ds-thread-count comments-count-link" data-thread-key="2017/03/05/随机森林算法/" data-count-type="comments">&nbsp;</a>
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/03/05/决策树和CART/" title="决策树和CART" itemprop="url">决策树和CART</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="jiayi797" target="_blank" itemprop="author">jiayi797</a>
		
  <p class="article-time">
    <time datetime="2017-03-05T04:22:25.000Z" itemprop="datePublished"> 发表于 2017-03-05</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h1 id="决策树简介">1. 决策树简介</h1><p>模仿人类决策的过程</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-05-12-43-33.png" alt=""> </p>
<ul>
<li>优点：好理解；简单</li>
<li>缺点：缺少很强的理论支持；树结构不唯一；</li>
</ul>
<h2 id="决策树的表达方式">1.1. 决策树的表达方式</h2><p>如上图所示的决策树，我们用$G(x)$来表达决策树：</p>
<p>$$G(x)=\sum_{t=1}^T q_t(x)\cdot g_t(x) $$</p>
<p>tips:</p>
<ul>
<li>$g(x)$是最终的决策（<code>Y or N</code>），叶子节点</li>
<li>$q_t(x)$是条件，<code>condition</code>。就是橘色箭头的判断过程</li>
<li>内部的决策过程，例如<code>deadline?</code>，内部节点</li>
</ul>
<p>那么决策树的表达就有两种方式：</p>
<ul>
<li><p>路径角度。将每个从根到叶子的路径作为一个假设g，通过不同的条件组合得到最后的G(X)。<br><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-05-12-53-15.png" alt=""> </p>
</li>
<li><p>递归角度。父树是由子树递归定义的<code>tree=(root,sub-trees)</code><br><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-05-12-53-25.png" alt=""> </p>
</li>
</ul>
<h2 id="基本流程">1.2. 基本流程</h2><ol>
<li>如何分支（branching criteria），即如何得到$b(x)$</li>
<li>根据分支，数据如何分块</li>
<li>根据数据，如何学习子树</li>
<li>得到最终的决策树</li>
</ol>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-05-15-33-51.png" alt=""> </p>
<h1 id="CART算法">2. CART算法</h1><ul>
<li>Classification And Regression Tree，分类回归树</li>
<li>二叉树（只有是、否）</li>
<li>输入：随机变量$X$</li>
<li>输出：随机变量$Y$的条件概率分布</li>
<li>$g_t(x)$返回一个常数（根据不同的条件，对数据进行切分，到达叶子节点时，根据剩下的数据进行预测，输出一个常数）</li>
</ul>
<h2 id="纯度">2.1. 纯度</h2><h3 id="纯度的定义">2.1.1. 纯度的定义</h3><ul>
<li>CART算法中每个节点（看做是一个决策桩decision stump）对数据进行切分，如果分出来的数据的y都很接近（回归问题）或者都一样（分类问题），那么我们说这样的数据是“纯的”，这样用标量对数据进行预测可以得到比较小的误差。</li>
</ul>
<p>CART分支$b(x)$为：<br><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-05-16-03-33.png" alt=""> </p>
<ul>
<li>我们通过上面的公式，来计算对于每一个节点的决策桩来说，分出来的两份数据的纯度是怎样的。</li>
<li>该公式通过计算数据集<code>Di（i=1 or 2）</code>的纯度并根据数据集的数量对其进行加权</li>
<li>其加权的意义是如果数据集的数量比较大的话，那个纯度对其比较重要</li>
<li>反之，就不那么重要。</li>
<li>CART通过分出的两部分数据综合起来的纯度对决策桩进行选择，选择“最纯”的分割方式作为当前的分支。</li>
</ul>
<h3 id="纯度的计算函数">2.1.2. 纯度的计算函数</h3><p>我们可以将分割出来的数据和回传的常数的误差作为评价纯度的方法，利用数据的y和回传的y_ba的均方误差来评价回归问题的纯度；利用0/1误差函数来评价分类问题的纯度。</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-05-16-11-16.png" alt=""> </p>
<p>如果是分类问题，我们还可以使用一个别的方法。通过基尼不纯度来度量分类问题的纯度问题。</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-05-16-11-41.png" alt=""> </p>
<h2 id="终止条件">2.2. 终止条件</h2><p>CART中有两种被迫终止的情况，分别是：</p>
<ul>
<li><code>yn</code>都一样，这时不纯度为0，于是可以得到<code>gt(x)=yn</code>；</li>
<li><code>xn</code>都一样，就没有继续分割的可能了。</li>
<li>CART树长到被迫停下来的情况，称为完全长成的树（fully-grown tree）。</li>
</ul>
<p>下面是CART算法完整流程：</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-05-16-19-09.png" alt=""> </p>
<h2 id="CART剪枝">2.3. CART剪枝</h2><p>预防过拟合</p>
<p><img src="http://om1bxijvl.bkt.clouddn.com/2017-03-05-16-20-48.png" alt=""> </p>
<p>上图告诉我们使用叶子的数目作为正则项（regularizer），最终得到一个正则化的决策树。<br>关于剪枝的具体做法时：</p>
<ul>
<li>首先得到完全长成的树作为<code>G(0)</code>；</li>
<li>然后试图摘掉一片叶子，将所有摘掉一片叶子后的树计算<code>Ein</code>，将最小的那棵摘掉一片叶子的数作为<code>G(1)</code>；</li>
<li>如此这般，得到摘掉两片叶子的最优树<code>G(2)</code>，这样不断剪枝，直到根结点，形成一个子树序列；</li>
<li>最终对这个子树序列使用<code>argmin Ein(G)+λΩ(G)</code>来得到最后的输出。</li>
</ul>
<h1 id="参考资料">3. 参考资料</h1><ol>
<li><a href="http://blog.jasonding.top/2015/07/22/Machine%20Learning/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95/" target="_blank" rel="external">Jason Ding，决策树算法</a></li>
<li>机器学习技法课程，林轩田，台湾大学</li>
</ol>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/机器学习/">机器学习</a>
  </div>

</div>




<div class="comments-count">
	
	  	<span></span>
		<a href="/2017/03/05/决策树和CART/#comments" class="ds-thread-count comments-count-link" data-thread-key="2017/03/05/决策树和CART/" data-count-type="comments">&nbsp;</a>
	
</div>

</footer>


    </article>







  <nav id="page-nav" class="clearfix">
    <a class="extend prev" rel="prev" href="/"><span></span>Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/3/">Next<span></span></a>
  </nav>

</div>
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  
<div class="github-card">
<p class="asidetitle">Github 名片</p>
<div class="github-card" data-github="jiayi797" data-width="220" data-height="119" data-theme="medium">
<script type="text/javascript" src="//cdn.jsdelivr.net/github-cards/latest/widget.js" ></script>
</div>
  </div>



  
<div class="categorieslist">
	<p class="asidetitle">分类</p>
		<ul>
		
		  
			<li><a href="/categories/ACM/" title="ACM">ACM<sup>3</sup></a></li>
		  
		
		  
			<li><a href="/categories/Java/" title="Java">Java<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/Numpy/" title="Numpy">Numpy<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/o2o优惠券使用预测/" title="o2o优惠券使用预测">o2o优惠券使用预测<sup>4</sup></a></li>
		  
		
		  
			<li><a href="/categories/数学/" title="数学">数学<sup>3</sup></a></li>
		  
		
		  
			<li><a href="/categories/机器学习/" title="机器学习">机器学习<sup>5</sup></a></li>
		  
		
		  
			<li><a href="/categories/瞎折腾/" title="瞎折腾">瞎折腾<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/算法/" title="算法">算法<sup>3</sup></a></li>
		  
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">标签</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/机器学习/" title="机器学习">机器学习<sup>5</sup></a></li>
			
		
			
				<li><a href="/tags/SQL/" title="SQL">SQL<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/测试/" title="测试">测试<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/数据预处理/" title="数据预处理">数据预处理<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/算法/" title="算法">算法<sup>1</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
          <li>
            
            	<a href="https://coderq.com" target="_blank" title="一个面向程序员交流分享的新一代社区">码农圈</a>
            
          </li>
        
          <li>
            
            	<a href="http://wuchong.me" target="_blank" title="Jark&#39;s Blog">Jark&#39;s Blog</a>
            
          </li>
        
    </ul>
</div>

  


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS 订阅</a>
</div>

  <div class="weiboshow">
  <p class="asidetitle">新浪微博</p>
    <iframe width="100%" height="119" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=119&fansRow=2&ptype=1&speed=0&skin=9&isTitle=1&noborder=1&isWeibo=0&isFans=0&uid=1145120523&verifier=a7c00b5e&dpc=1"></iframe>
</div>


</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Nothing lasts forever. <br/>
			</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		<a href="http://weibo.com/1145120523" target="_blank" class="icon-weibo" title="微博"></a>
		
		
		<a href="https://github.com/jiayi797" target="_blank" class="icon-github" title="github"></a>
		
		
		
		
		
		
		
		
		
		<a href="mailto:jiayi797@163.com" target="_blank" class="icon-email" title="Email Me"></a>
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2017 
		
		<a href="/about" target="_blank" title="jiayi797">jiayi797</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
        
    }
  });
});
</script>










<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?9856596edaab494b299151eb0e9bb214";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>



<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="返回顶部"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End --><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"]]}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

  </body>
 </html>
